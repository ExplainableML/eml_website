---
title: Distillation of Vision Language Models
description: Vision Language Models, with the Large Language Models as the backbone, have showcased impressive skills in tasks related to visual understanding and reasoning.  Yet, their widespread application faces obstacles due to the high computational demands during both training and inference phases.  A common technique to reduce high computational resource demand is knowledge distillation. We aim to explore how to distill the knowledge from larger models into smaller models.
contactname: Yiran Huang
contactlink: /people/yiran-huang