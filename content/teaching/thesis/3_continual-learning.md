---
title: Continual Learning with Pretrained Models
description: Continual Learning (CL) was originally used to train models from scratch on a stream of tasks. However, with the rise of ubiquitous pretrained foundation models, the focus in CL has shifted to continually adapting these pretrained models for downstream tasks. Parameter Efficient Fine Tuning (PEFT) has proven highly successful in this regard. Nonetheless, several open challenges remain. Our aim is to explore how PEFT can be used in a continual or lifelong learning setting to adapt a pretrained model to various downstream tasks.
contactname: Lukas Thede
contactlink: /people/lukas-thede