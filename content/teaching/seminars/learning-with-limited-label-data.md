---
title: Seminar Learning with Limited Label Data
---

# Seminar: Learning with Limited Label Data (WS 21 / 22)
## Content

Current publications on machine learning / computer vision are covered in this seminar. In this seminar, we will discuss research papers related to visual learning with limited labeled data, including deep learning methods for zero-shot and few-shot learning, semi-supervised learning, unsupervised pre-training, and self-supervised learning.

This is a Master's level course. Since these topics are very complex, prior participation in at least one of the following lectures is required:

- General knowledge on Statistical Machine Learning
- General knowledge on Deep Learning
- General knowledge on Computer Vision
 

## Organisation

The schedule of the seminar is as follows:

- October 22th, 2-4pm            
- October 29th, 2-4pm            
- November 5th, 2-4pm            
- November 12th, 2-4pm            
- November 19th, 2-6pm            
- November 26th, 2-6pm            
- December 3rd, 2-6pm            
- December 10th, 2-6pm            
- December 17th, 2-6pm

All seminars will take place on zoom. All the accepted participants will receive the zoom link on their email that they used in ILIAS.

The course awards 3 LP Credits.

 

## Requirements

A successful participation in the seminar includes:

- Active participation in the entire event: We have a 70% attendance policy for this seminar. You need to attend at least 7 of the 9 sessions.
- Short presentation on October 29th, November 5, 12, 19th (10 minutes talk, 5 min questions)
- Presentation on November 26th, December 3, 10 or 17th (20 minutes talk, 10 minutes questions) on a selected topic

## Topics to be covered

### Semi-supervised learning

- [Temporal Ensembling for Semi-Supervised Learning](https://arxiv.org/abs/1610.02242), Laine et al., ICLR2017
- [Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results](https://arxiv.org/abs/1703.01780), Tarvainen et al., NeuRIPS2017
- [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907), Kipf et al., ICLR2017
- [Smooth Neighbors on Teacher Graphs for Semi-supervised Learning](https://arxiv.org/abs/1711.00258), Luo et al., CVPR2018
- [Deep Co-Training for Semi-Supervised Image Recognition](https://arxiv.org/abs/1803.05984), Qiao et al., ECCV2018
- [ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring](https://arxiv.org/abs/1911.09785?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529), Berthelot et al., ICLR2020

### Self-supervised learning/Unsupervised pre-training

- [Deep Clustering for Unsupervised Learning of Visual Features](https://arxiv.org/abs/1807.05520), Caron et al,. ECCV2018
- [Self-Supervised GANs via Auxiliary Rotation Loss](https://arxiv.org/abs/1811.11212), Chen et at., CVPR2019
- [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709), Chen et al,. ICML2020
- [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722), He et al,. CVPR2020
- [Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566), Chen et al., CVPR2021
- [Bootstrap Your Own Latent A New Approach to Self-Supervised Learning](https://arxiv.org/abs/2006.07733?fileGuid=WyYwxqq8kWjKdWgd), Grill et al,. NeuRIPS2020

### Low-shot Learning (zero-shot/few-shot learning)

- [Prototypical Networks for Few-Shot Learning](https://arxiv.org/abs/1703.05175), Snell et al., NeurIPS 2017.
- [Dynamic Few-Shot Visual Learning without Forgetting](https://arxiv.org/abs/1804.09458), Gidaris and Komodakis, CVPR 2018.
- [Learning to Compare: Relation Network for Few-Shot Learning](https://arxiv.org/abs/1711.06025), Sung et al., CVPR 2018.
- [Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly](https://arxiv.org/abs/1707.00600), Xian et al., TPAMI 2018.
- [Attribute Prototype Network for Zero-Shot Learning](https://arxiv.org/abs/2008.08290), Xu et al., NeurIPS 2020.
- [f-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning](https://openaccess.thecvf.com/content_CVPR_2019/papers/Xian_F-VAEGAN-D2_A_Feature_Generating_Framework_for_Any-Shot_Learning_CVPR_2019_paper.pdf) Xian et al., CVPR 2019.

### Cross-domain learning

- [Unsupervised Domain Adaptation by Backpropagation](https://arxiv.org/abs/1409.7495), Ganin et al., ICML 2015.
- [AutoDIAL: Automatic DomaIn Alignment Layers](https://arxiv.org/abs/1704.08082), Carlucci et al., ICCV 2017.
- [CyCADA: Cycle-Consistent Adversarial Domain Adaptation](https://arxiv.org/abs/1711.03213), Hoffman et al., ICML 2018.
- [Self-Ensembling for Visual Domain Adaptation](https://arxiv.org/abs/1706.05208), French et al., ICLR 2018.
- [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1712.02560), Saito et al., CVPR 2018.
- [Domain Generalization by Solving Jigsaw Puzzles](https://arxiv.org/abs/1903.06864), Carlucci et al., CVPR 2019.

### Emerging trends

- [Semi-Supervised Learning under Class Distribution Mismatch](https://xiatian-zhu.github.io/papers/ChenEtAl_AAAI2020.pdf), Chen et al., AAAI2020
- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction](https://arxiv.org/abs/2103.03230), Zbontar et al,. ICML2021
- [Towards Recognizing Unseen Categories in Unseen Domains](https://arxiv.org/abs/2007.12256), Mancini et al., ECCV 2020.
- [Universal Domain Adaptation](https://openaccess.thecvf.com/content_CVPR_2019/papers/You_Universal_Domain_Adaptation_CVPR_2019_paper.pdf), You et al., CVPR 2019.
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020), Radford et al,. ICML2021
- [Learning Graph Embeddings for Compositional Zero-Shot Learning](https://arxiv.org/abs/2102.01987), Naeem et al.,CVPR 2021.
 
## Registration

The registration opens on October 1st via [ILIAS](https://ovidius.uni-tuebingen.de/ilias3/goto.php?target=cat_3279795&client_id=pr02).