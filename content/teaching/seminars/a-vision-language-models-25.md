---
title: Advanced Topics in Vision-Language Models (SS 2025)
---
# TUM Master's Seminar: Advanced Topics in Vision-Language Models (SS 2025)

## Content

</br>

The seminar aims to explore cutting-edge advancements in the realm of Vision-Language Models (VLMs), focusing on various topics crucial to their development and application. Through a deep dive into seminal papers and latest research, students will gain an understanding of how models like CLIP, Llama, and Stable Diffusion work at an architectural and mathematical level. By the end of the seminar, students should have a comprehensive perspective on the current state and future potential of vision-language modeling. They will be equipped to evaluate new research, identify promising applications, and contribute meaningfully to the responsible development of this important field.

</br>

This is a Master's level course. Since these topics are very complex, prior participation in at least one of the following lectures is required:
- Introduction to Deep Learning (IN2346)
- Machine Learning (IN2064)

</br>

Additionally, we recommend to have taken at least one advanced deep learning lecture, for example:
- AML: Deep Generative Models (CIT4230003)
- Machine Learning for Graphs and Sequential Data (IN2323)
- Computer Vision III: Detection, Segmentation, and Tracking (IN2375)
- Machine Learning for 3D Geometry (IN2392)
- Advanced Natural Language Processing (CIT4230002)
- ADL4CV (IN2390)
- ADL4R (IN2349)
or a related practical.

## Organization

</br>

The preliminary meeting will take place at 2pm on Wednesdy, 12th of Februray 2025 on [Zoom](https://tum-conf.zoom-x.de/j/8713460070?pwd=SEd6a2QycjF0VnMzOGVDOXpuNzkwQT09). See [Slides](https://drive.google.com/file/d/1INDmalibspXghQrTBPgGS65rygeZ9sjJ/view?usp=drive_link).

</br>

The seminar awards 5 ECTS Credits and will take place in person at [SAP Labs Munich](https://www.google.com/maps/place/SAP+Labs+Munich+(MUE03)/@48.2637921,11.6667211,15.71z/data=!4m6!3m5!1s0x479e73ae34549fab:0xcf849163c0750852!8m2!3d48.2641469!4d11.6610809!16s%2Fg%2F11s93pl1ct?entry=ttu&g_ep=EgoyMDI1MDMwMi4wIKXMDSoJLDEwMjExNDUzSAFQAw%3D%3D) in Garching campus.

</br>

All students will be matched to __one topic group including a primary paper and two secondary papers__. They are expected to give one short and one long presentation on their primary paper (from the perspective of an __academic reviewer__) as well as a one-slide on the secondary papers from two different perspectives (__industry practitioner__ and __academic researcher__).

</br>

The tentative schedule of the seminar is as follows:
- __Online__ Introductory Session, April 9th, (1-2:30pm), [Zoom](https://tum-conf.zoom-x.de/j/64655141614?pwd=3iLEwrAUhs369HEbTS3waR1mdbv5Ws.1),[slides](https://drive.google.com/file/d/11p-U8RBOeRyeIaNBcJlm2QoDYdI51zS1/view?usp=sharing)    
- __Onsite__ Short presentations on April 23rd (Room B2.01) and 30th (Room B1.01) (1-3pm)
- __Onsite__ Long presentations on May 21st (Room B1.01), May 28th (Room B1.01), June 4th (Room B1.01), 11th (Room B1.01), July 2nd (Room B2.01) (1-3pm)

</br>

For questions, please contact yiran.huang@helmholtz-munich.de or sanghwan.kim@helmholtz-munich.de.

</br>

# Topics to select from:

### Foundation VLMs

</br>

1. [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
2. [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)
3. [High-Resolution Image Synthesis With Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
4. [JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2411.07975)

</br>

### RLHF in Vision Language Models

</br>

1. [MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning](https://arxiv.org/abs/2503.07365)
2. [Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/abs/2503.06749)
3. [KTO: Model Alignment as Prospect Theoretic Optimization](https://arxiv.org/abs/2402.01306)
4. [Diffusion Model Alignment Using Direct Preference Optimization](https://arxiv.org/abs/2311.12908)

</br>

### Applications of T2I Models

</br>

1. [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)
2. [ReNoise: Real Image Inversion Through Iterative Noising](https://arxiv.org/abs/2403.14602)
3. [DataDream: Few-shot Guided Dataset Generation](https://arxiv.org/abs/2407.10910)
4. [DiG-IN: Diffusion Guidance for Investigating Networks -- Uncovering Classifier Differences Neuron Visualisations and Visual Counterfactual Explanations](https://arxiv.org/abs/2311.17833)

</br>

### Concept-based Explainability 

</br>

1. [Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery](https://arxiv.org/abs/2407.14499)
2. [Large Multi-modal Models Can Interpret Features in Large Multi-modal Models](https://arxiv.org/abs/2411.14982)
3. [PDiscoNet: Semantically consistent part discovery for fine-grained recognition](https://arxiv.org/abs/2309.03173)
4. [Sparse Autoencoders Find Highly Interpretable Features in Language Models ](https://arxiv.org/abs/2309.08600)
</br>

### Compositionality

</br>

1. [When and why vision-language models behave like bags-of-words, and what to do about it?](https://arxiv.org/abs/2210.01936)
2. [Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs](https://arxiv.org/abs/2401.06209)
3. [Language-only Efficient Training of Zero-shot Composed Image Retrieval](https://arxiv.org/abs/2312.01998)
4. [Imagine and Seek: Improving Composed Image Retrieval with an Imagined Proxy](https://arxiv.org/abs/2411.16752)


</br>

## Requirements

</br>

A successful participation in the seminar includes:
- Active participation in the entire event: We have 70% attendance policy for this seminar. (You need to attend at least 5 of the 7 sessions.)
- Short presentation (10 minutes talk including questions)
- Long presentation (20 minutes talk including questions)

## Registration

</br>

The registration must be done through the [TUM Matching Platform](https://matching.in.tum.de/).