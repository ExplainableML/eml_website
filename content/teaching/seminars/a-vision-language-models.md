---
title: Advanced Topics in Vision-Language Models
---
# TUM Master's Seminar: Advanced Topics in Vision-Language Models (SS 2024)

## Content

</br>

The seminar aims to explore cutting-edge advancements in the realm of Vision-Language Models (VLMs), focusing on various topics crucial to their development and application. Through a deep dive into seminal papers and latest research, students will gain an understanding of how models like CLIP, Llama, and Stable Diffusion work at an architectural and mathematical level. By the end of the seminar, students should have a comprehensive perspective on the current state and future potential of vision-language modeling. They will be equipped to evaluate new research, identify promising applications, and contribute meaningfully to the responsible development of this important field.

</br>

This is a Master's level course. Since these topics are very complex, prior participation in at least one of the following lectures is required:
- Introduction to Deep Learning (IN2346)
- Machine Learning (IN2064)

</br>

Additionally, we recommend to have taken at least one advanced deep learning lecture, for example:
- AML: Deep Generative Models (CIT4230003)
- Machine Learning for Graphs and Sequential Data (IN2323)
- Computer Vision III: Detection, Segmentation, and Tracking (IN2375)
- Machine Learning for 3D Geometry (IN2392)
- Advanced Natural Language Processing (CIT4230002)
- ADL4CV (IN2390)
- ADL4R (IN2349)
or a related practical.

# Topics to select from:

</br>

### Foundation VLMs

1. Learning Transferable Visual Models From Natural Language Supervision (CLIP, https://arxiv.org/abs/2103.00020)
2. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (https://arxiv.org/abs/2201.12086)
3. Visual Instruction Tuning (https://arxiv.org/abs/2304.08485)
4. CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation (https://arxiv.org/pdf/2311.18775.pdf)
5. Hierarchical Text-Conditional Image Generation with CLIP Latents (DALLE-2, https://arxiv.org/pdf/2204.06125.pdf)
6. High-Resolution Image Synthesis With Latent Diffusion Models (Stable Diffusion, https://arxiv.org/abs/2112.10752)
7. Aligning Large Multimodal Models with Factually Augmented RLHF (https://arxiv.org/abs/2309.14525)
8. MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices (https://arxiv.org/abs/2312.16886)

</br>

### Zero/few-shot learning in VLMs

1. Test-time Adaptation with CLIP Reward for Zero-shot Generalization in VLMs (https://arxiv.org/pdf/2305.18010.pdf)
2. Prompt Distribution Learning (https://arxiv.org/pdf/2205.03340.pdf)
3. Large Multilingual Models Pivto Zero-shot Multimodal Learning Across Languages (https://arxiv.org/pdf/2308.12038.pdf)
4. Text-to-image diffusion models are zero-shot classifiers (https://arxiv.org/pdf/2303.15233.pdf)
5. DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models (https://arxiv.org/pdf/2310.16436.pdf)
6. Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners (https://arxiv.org/pdf/2303.02151.pdf)
7. Waffling around for Performance: Visual Classification with Random Words and Broad Concepts (https://arxiv.org/pdf/2306.07282.pdf)
8. Learning Vision from Models Rivals Learning Vision from Data (https://arxiv.org/pdf/2312.17742.pdf)

</br>

### Language guidance in computer vision

1. DeViL: Decoding Vision features into Language (https://arxiv.org/abs/2309.01617)
2. Label-Free Concept Bottleneck Models (https://arxiv.org/abs/2304.06129)
3. Generalized Category Discovery (https://arxiv.org/abs/2201.02609)
4. Visual Classification via Description from Large Language Models (https://arxiv.org/abs/2210.07183)
5. ViperGPT: Visual Inference via Python Execution for Reasoning (https://arxiv.org/abs/2303.08128)
6. Integrating Language Guidance into Vision-based Deep Metric Learning (https://arxiv.org/abs/2203.08543)
7. Image-free Classifier Injection for Zero-Shot Classification (https://arxiv.org/abs/2308.10599)
8. Segment Anything (https://arxiv.org/abs/2304.02643)

</br>

### Personalized text2image models

1. Adding Conditional Control to Text-to-Image Diffusion Models (ControlNet, https://arxiv.org/abs/2302.05543)
2. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion (https://textual-inversion.github.io/)
3. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation (https://dreambooth.github.io/)
4. Image Sculpting: Precise Object Editing with 3D Geometry Control (https://image-sculpting.github.io/)
5. Differential Diffusion: Giving Each Pixel Its Strength (https://differential-diffusion.github.io/)
6. Transparent Image Layer Diffusion using Latent Transparency (https://arxiv.org/pdf/2402.17113v1.pdf)
7. Manifold Preserving Diffusion Models (https://arxiv.org/pdf/2311.16424.pdf)
8. Anti-DreamBooth: Protecting users from personalized text-to-image synthesis (https://arxiv.org/pdf/2303.15433.pdf)

</br>

### Compositionality

1. Vision-by-Language for Training-Free Compositional Image Retrieval (https://arxiv.org/abs/2310.09291)
2. CoVR: Learning Composed Video Retrieval from Web Video Captions (https://imagine.enpc.fr/~ventural/covr/dataset/covr.pdf)
3. Zero-Shot Composed Image Retrieval with Textual Inversion (https://arxiv.org/abs/2303.15247)
4. Compositional Chain-of-Thought Prompting for Large Multimodal Models (https://arxiv.org/abs/2311.17076)
5. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality (https://proceedings.neurips.cc/paper_files/paper/2023/file/63461de0b4cb760fc498e85b18a7fe81-Paper-Datasets_and_Benchmarks.pdf)
6. Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models (https://arxiv.org/abs/2301.13826)
7. Visual Programming: Compositional visual reasoning without training (https://arxiv.org/abs/2211.11559)
8. When and why vision-language models behave like bags-of-words, and what to do about it? (https://arxiv.org/abs/2210.01936)

</br>

## Organization

</br>

The preliminary meeting will take place at 1pm on Thursday, 8th of Februray 2024 on Zoom [https://zoom.us/j/92673399783](https://zoom.us/j/92673399783) . You can finde the slides [here](https://drive.google.com/file/d/1ji2Sp1J-Xtoki5PiuumrQ1LEYfTYdqGl/view?usp=sharing).

</br>

The seminar awards 5 ECTS Credits and will take place in person at [Helmholtz AI Munich](https://www.google.com/maps?ll=48.220675,11.596054&z=17&t=m&hl=en&gl=GB&mapclient=embed&cid=3379363886196135068).

</br>

All students will be matched to __one topic group including a primary paper and two secondary papers__. They are expected to give one short and one long presentation on their primary paper (from the perspective of an __academic reviewer__) as well as a one-slide on the secondary papers from two different perspectives (__industry practicioner__ and __acedemic researcher__).

</br>

The tentative schedule of the seminar is as follows:
- Online Introductory Session, April 18th, (1-2:30pm)
- Short presentations on May 16th, 23rd (1-3pm)
- Long presentations on June 6th, 13th, 20th, 27th, July 4th (1-3pm)

</br>

For questions, please contact yiran.huang@helmholtz-munich.de or luca.eyring@helmholtz-munich.de.

## Requirements

</br>

A successful participation in the seminar includes:
- Active participation in the entire event: We have 70% attendance policy for this seminar. (You need to attend at least 5 of the 7 sessions.)
- Short presentation on May 16th or 23rd (10 minutes talk including questions)
- Long presentation on June 6th, 13th, 20th, 27th, or July 4th (20 minutes talk including questions)

## Registration

</br>

The registration must be done through the [TUM Matching Platform](https://matching.in.tum.de/).