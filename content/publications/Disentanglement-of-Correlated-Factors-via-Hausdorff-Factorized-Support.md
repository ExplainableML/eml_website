---
img: "/publications/ICLR23_Hausdorff/first_page.png"
title: Disentanglement of Correlated Factors via Hausdorff Factorized Support
authors: Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent*, Diane Bouchacourt*
publisher: International Conference on Learning Representations, ICLR
year: 2023
date: "2023-01-20"
filename: Disentanglement-of-Correlated-Factors-via-Hausdorff-Factorized-Support
arxiv: https://arxiv.org/abs/2210.07347
github: https://arxiv.org/abs/2210.07347

abstract: A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts. Disentanglement is one promising direction aimed at aligning a models representations with the underlying factors generating the data (e.g. color or background). Existing disentanglement methods, however, rely on an often unrealistic assumption - that factors are statistically independent. In reality, factors (like object color and shape) are correlated. To address this limitation, we propose a relaxed disentanglement criterion - the Hausdorff Factorized Support (HFS) criterion - that encourages a factorized support, rather than a factorial distribution, by minimizing a Hausdorff distance. This allows for arbitrary distributions of the factors over their support, including correlations between them. We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over +60% in relative improvement over existing disentanglement methods. In addition, we find that leveraging HFS for representation learning can even facilitate transfer to downstream tasks such as classification under distribution shifts. We hope our original approach and positive empirical results inspire further progress on the open problem of robust generalization.
---

# Introduction
Disentangled representation learning is a promising path to facilitate reliable generalization to in- and out-of-distribution downstream tasks, on top of being more interpretable and fair (Bengio et al. 2013, Higgins et al. 2018, Locatello et al. 2019,2020). 
While various metrics have been proposed to measure disentanglement, the most commonly understood definition is as follows:   

</br>

__Definition.__ _Assuming the data has been generated by a set of unknown, ground-truth latent factors, a representation is said to be disentangled if each factor is recovered in one and only one dimension of the representation._   

</br>

The method by which to achieve this goal however, remains an open research question. 
Weak and semi-supervised settings, e.g. using paired data samples or auxiliary variables, can provably offer disentanglement and recovery of ground truth factors (e.g. Bouchacourt et al. 2018, Locatello et al. 2020, ...).
But fully unsupervised disentanglement -- _our focus in this study_ -- is in theory impossible to achieve in the general unconstrained nonlinear case (c.f. Hyvaerinen et al. 1999, Locatello et al. 2019).
In practice however, the inductive biases embodied in common autoencoder architectures allow for effective _practical_ disentanglement (Rolinek et al. 2019).

</br>

Perhaps more problematic, standard unsupervised disentanglement methods (s.a. $\beta$-(TC)VAE, AnnealedVAE, DIP-VAE, ...) rely on an unrealistic assumption of statistical independence of ground truth factors. Real data however contains correlations (Traeuble et al. 2021).
Even _with well defined_ factors, such as object shape, color or background, correlations are pervasive: yellow bananas are more frequent than red ones; and cows are much more often on pasture than sand dunes.
In more realistic settings where factors are correlated, prior work has shown existing disentanglement methods fail.

</br>

To address this limitation, we propose to _relax_ the unrealistic assumption of statistical independence of factors (i.e. that they have a factorial distribution), and only assume the support of the factors' distribution factorizes -- a much weaker, but more realistic constraint.
To visualize this, consider a dataset of animal images where background and animal type are heavily correlated (camels most likely on sand, and cows on grass):   

</br>
</br>

![text](/publications/ICLR23_Hausdorff/first_page.png)

</br>
</br>

Under the original assumption of factor independence, a model likely learns a shortcut solution where animal and landscape share the same latent correspondence (Beery et al. 2018).
On the other hand with a factorized support, learned factors should be such that any combination of their values has some grounding in reality: a cow on sand is an unlikely, yet not impossible combination.
We still rely, just as standard unsupervised disentanglement methods, on the inductive bias of encoder-decoder architectures to recover factors -- however, we expect our method to facilitate _robustness to any distribution shifts within the support_, as it makes no assumptions on the distribution beyond its factorized support. 


# Method
On this basis, we propose a concrete pairwise Hausdorff Factorized Support (__HFS__) training criterion to disentangle correlated factors, by aiming for all pairs of latents to have a factorized support.
Specifically, we encourage a factorized support by minimizing a Hausdorff set-distance between the finite sample approximation of the actual support and its factorization (c.f. Huttenlocher et al. 1993, Rockafellar et al. 1998).

We are given a dataset $\mathcal{D}=\{\mathbf{x}^i\}_{i=1}^{N}$ (e.g. \removed{of }images), where each $\mathbf{x}^i$ is a realization of a random variable, e.g., an image. We consider that each $\mathbf{x}^i$ is generated by an unknown generative process, \replaced{which we suppose involves}{involving} a ground truth latent random vector $\mathbf{z}$ whose components correspond to the dataset's underlying factors of variations (s.a. object shape, \removed{pose, }color, background, \ldots). This process generates an observation $\mathbf{x}$, by first drawing a realization $\mathbf{z}=(z_1,\ldots,z_k)$ from a distribution $p(\mathbf{z})$, i.e. $\mathbf{z} \sim p(\mathbf{z})$. Observation $\mathbf{x}$ is then obtained by drawing $\mathbf{x} \sim p(\mathbf{x}|\mathbf{z})$. 

Given $\mathcal{D}$, the _goal_ of disentangled representation learning can be stated as \removed{that of }learning a mapping $f_\phi$ that for any $\mathbf{x}$ recovers as best as possible the associated $\mathbf{z}$ i.e. $f_\phi(\mathbf{x}) \approx \mathbb{E}[\mathbf{z}| \mathbf{x}]$ up to \added{a} permutation of \removed{the }elements and elementwise bijective transformation.

In unsupervised disentanglement, the $\mathbf{z}$ are unobserved, and both $p(\mathbf{z})$ and $p(\mathbf{x}|\mathbf{z})$ are _a priori unknown to us_, though we might assume specific properties and functional forms.

Most unsupervised disentanglement methods follow the formalization of VAEs and employ parameterized _probabilistic generative models_ of the form $p_\theta(\mathbf{x}, \mathbf{z}) = p_\theta(\mathbf{z}) p_\theta(\mathbf{x} | \mathbf{z})$ to estimate the ground truth generative model over $\mathbf{z},\mathbf{x}$. As in VAEs, these methods make the strong assumption that ground truth factors are statistically independent,

$p(\mathbf{z})=p(z_1) p(z_2) \ldots p(z_k),

and conflate the goal of learning a disentangled representation with that of learning a representation with statically independent components. This assumption naturally translates to a factorial model prior $p_\theta(\mathbf{z})$.



Instead of assuming independent factors (i.e. a factorial distribution on $\mathbf{z}$ as in Eq.~\ref{eq:fact-distr}), we will only assume that the _support_ of the distribution factorizes. Let us denote by $\mathcal{S}(p(\mathbf{z}))$ the _support_ of $p(\mathbf{z})$, i.e. the set $\{\mathbf{z} \in \mathcal{Z} \,|\, p(\mathbf{z}) > 0 \}$.
We say that $\mathcal{S}(p(\mathbf{z}))$ is factorized if it equals to the Cartesian product of supports over individual dimensions' marginals, i.e. if:

$\mathcal{S}(p(\mathbf{z})) = \mathcal{S}(p(z_1))\times\mathcal{S}(p(z_2))\times ... \times\mathcal{S}(p(z_k)) \stackrel{\text{def}}{=}  \mathcal{S}^X(p(\mathbf{z}))$

where $\times$ denotes the Cartesian product.


Of course, independence implies factorized-support, but not the other way around independence - assuming a factorized support is thus a _relaxation_ of the (unrealistic) assumption of factorial distribution, (i.e. of statistical independence) of disentangled factors. Refer to the previous cartoon example, where the distribution of the two disentangled factors would not satisfy an independence assumption, but does have a factorized support. Informally the factorized support assumption is merely stating that whatever values $z_1$ and $z_2$, etc... may take individually, any combination of these is _possible_ (even when not very likely).


Let us consider deterministic _representations_ obtained by the encoder $\mathbf{z}=f_\phi(\mathbf{x})$.
We enforce the factorial support criterion on the aggregate distribution $\bar{q}_\phi(\mathbf{z})=\mathbb{E}_\mathbf{x}[f_\phi(\mathbf{x})]$, where $\bar{q}_\phi(\mathbf{z})$ is conceptually similar to the _aggregate posterior_ $q_\phi(\mathbf{z})$ in e.g. $\beta$-TCVAE, though we consider points produced by a deterministic mapping $f_\phi$ rather than a stochastic one.

To guide the learning, we thus need a divergence or metric to tell us how far $\mathcal{S}$ is from $\mathcal{S}^X$. Supports are sets, so it is natural to use a set distance such as the Hausdorff distance, giving 

$d_H(\mathcal{S}, \mathcal{S}^X) = \max\left(\sup_{\mathbf{z}\in\mathcal{S}^X}\left[\inf_{\mathbf{z}'\in\mathcal{S}} d(\mathbf{z},\mathbf{z}')\right], \sup_{\mathbf{z}\in\mathcal{S}}\left[\inf_{\mathbf{z}'\in\mathcal{S}^X} d(\mathbf{z},\mathbf{z}')\right]\right) =\sup_{\mathbf{z}\in\mathcal{S}^X}\left[\inf_{\mathbf{z}'\in\mathcal{S}} d(\mathbf{z},\mathbf{z}')\right]$

with the second part of the Hausdorff distance equating to zero since $\mathcal{S}\subset\mathcal{S}^X$. 

In practial settings with a finite sample of observations $\{\mathbf{x}\}_i^N$, we further introduce a practical Monte-Carlo batch-approximation: with access to a batch of $b$ inputs $\mathbf{X}$ yielding $b$ $k$-dimensional latent representations $\mathbf{Z} = f_\phi(\mathbf{X})\in\mathbb{R}^{b\times k}$, we estimate Hausdorff distances using sample-based approximations to the support: $\mathcal{S} \approx \mathbf{Z}$ and $\mathcal{S}^X \approx \mathbf{Z}_{:, 1}\times\mathbf{Z}_{:, 2}\times...\times\mathbf{Z}_{:, k} = \{ (z_1, \ldots, z_k),\; z_1 \in \mathbf{Z}_{:, 1}, \ldots, z_k \in \mathbf{Z}_{:, k} \}$. Here $\mathbf{Z}_{:, j}$ must be understood as the _set_ (not vector) of all elements in the $j^\mathrm{th}$ column of $\mathbf{Z}$. Plugging into above equation yields:


$\textstyle\hat{d}_{H}(\mathbf{Z}) = \max_{\mathbf{z}\in \mathbf{Z}_{:, 1}\times\mathbf{Z}_{:, 2}\times...\times\mathbf{Z}_{:, k}} \left[\min_{\mathbf{z}'\in \mathbf{Z}} d(\mathbf{z},\mathbf{z}')\right]$


where by noting $\mathbf{z}' \in \mathbf{Z}$ we consider the matrix $\mathbf{Z}$ as a \emph{set} of rows.



In high dimension, with many factors, the assumption that _every combination of all latent values is possible_ might still be too strong an assumption. And even if we assumed all to be in principle possible, we can never hope to observe all in a finite dataset of realistic size due to the combinatorial explosion of conceivable combinations. However, it is statistically reasonable to expect evidence of a factorized support for all _pairs_ of elements.
To encourage such a pairwise factorized support, we minimize a sliced, pairwise Hausdorff estimate with the additional benefit of keeping computation tractable when $k$ is large:

$\textstyle\hat{d}^{(2)}_{H}(\mathbf{Z}) = \sum_{i=1}^{k-1}\sum_{j=i+1}^k\max_{\mathbf{z}\in\mathbf{Z}_{:,i}\times\mathbf{Z}_{:,j}} \left[\min_{\mathbf{z}'\in \mathbf{Z}_{:, (i,j)}} d(\mathbf{z},\mathbf{z}')\right]$

where $\mathbf{Z}_{:, (i,j)}$ denotes the concatenation of column $i$ and column $j$, yielding a \emph{set of rows}.


# Experiments
Across large-scale experiments on standard disentanglement benchmarks and novel extensions with correlated factors, __HFS__ consistently facilitates disentanglement.
We also show that __HFS__ can be implemented as a regularizer alongside other disentanglement methods to reliably improve disentanglement.
In particular, we observe significant improvements of in parts over $+60\%$ in disentanglement performance over baselines as measured by DCI-D (Eastwood et al. 2018)

</br>

![text](/publications/ICLR23_Hausdorff/main_table.png)

</br>

In addition, on downstream classification tasks, we show improved generalization to more severe distribution shifts as well as improved sample efficiency.

</br>

![text](/publications/ICLR23_Hausdorff/ood_corr_matrix_figure.png)

</br>

</br>

![text](/publications/ICLR23_Hausdorff/relative_improvement_gbt.png)

</br>
