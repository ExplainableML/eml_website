---
img: "/publications/ICLR23_Hausdorff/first_page.png"
title: Disentanglement of Correlated Factors via Hausdorff Factorized Support
authors: Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent*, Diane Bouchacourt*
publisher: International Conference on Learning Representations, ICLR
year: 2023
date: "2023-01-20"
filename: Disentanglement-of-Correlated-Factors-via-Hausdorff-Factorized-Support
arxiv: https://arxiv.org/abs/2210.07347
github: https://arxiv.org/abs/2210.07347

abstract: A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts. Disentanglement is one promising direction aimed at aligning a models representations with the underlying factors generating the data (e.g. color or background). Existing disentanglement methods, however, rely on an often unrealistic assumption - that factors are statistically independent. In reality, factors (like object color and shape) are correlated. To address this limitation, we propose a relaxed disentanglement criterion - the Hausdorff Factorized Support (HFS) criterion - that encourages a factorized support, rather than a factorial distribution, by minimizing a Hausdorff distance. This allows for arbitrary distributions of the factors over their support, including correlations between them. We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over +60% in relative improvement over existing disentanglement methods. In addition, we find that leveraging HFS for representation learning can even facilitate transfer to downstream tasks such as classification under distribution shifts. We hope our original approach and positive empirical results inspire further progress on the open problem of robust generalization.
---

# Introduction
Disentangled representation learning is a promising path to facilitate reliable generalization to in- and out-of-distribution downstream tasks, on top of being more interpretable and fair (Bengio et al. 2013, Higgins et al. 2018, Locatello et al. 2019,2020). 
While various metrics have been proposed to measure disentanglement, the most commonly understood definition is as follows:   

</br>

__Definition.__ _Assuming the data has been generated by a set of unknown, ground-truth latent factors, a representation is said to be disentangled if each factor is recovered in one and only one dimension of the representation._   

</br>

The method by which to achieve this goal however, remains an open research question. 
Weak and semi-supervised settings, e.g. using paired data samples or auxiliary variables, can provably offer disentanglement and recovery of ground truth factors (e.g. Bouchacourt et al. 2018, Locatello et al. 2020, ...).
But fully unsupervised disentanglement -- _our focus in this study_ -- is in theory impossible to achieve in the general unconstrained nonlinear case (c.f. Hyvaerinen et al. 1999, Locatello et al. 2019).
In practice however, the inductive biases embodied in common autoencoder architectures allow for effective _practical_ disentanglement (Rolinek et al. 2019).

</br>

Perhaps more problematic, standard unsupervised disentanglement methods (s.a. $\beta$-(TC)VAE, AnnealedVAE, DIP-VAE, ...) rely on an unrealistic assumption of statistical independence of ground truth factors. Real data however contains correlations (Traeuble et al. 2021).
Even _with well defined_ factors, such as object shape, color or background, correlations are pervasive: yellow bananas are more frequent than red ones; and cows are much more often on pasture than sand dunes.
In more realistic settings where factors are correlated, prior work has shown existing disentanglement methods fail.

</br>

To address this limitation, we propose to _relax_ the unrealistic assumption of statistical independence of factors (i.e. that they have a factorial distribution), and only assume the support of the factors' distribution factorizes -- a much weaker, but more realistic constraint.
To visualize this, consider a dataset of animal images where background and animal type are heavily correlated (camels most likely on sand, and cows on grass):   

</br>
</br>

![text](/publications/ICLR23_Hausdorff/first_page.png)

</br>
</br>

Under the original assumption of factor independence, a model likely learns a shortcut solution where animal and landscape share the same latent correspondence (Beery et al. 2018).
On the other hand with a factorized support, learned factors should be such that any combination of their values has some grounding in reality: a cow on sand is an unlikely, yet not impossible combination.
We still rely, just as standard unsupervised disentanglement methods, on the inductive bias of encoder-decoder architectures to recover factors -- however, we expect our method to facilitate _robustness to any distribution shifts within the support_, as it makes no assumptions on the distribution beyond its factorized support. 


# Method
On this basis, we propose a concrete pairwise Hausdorff Factorized Support (__HFS__) training criterion to disentangle correlated factors, by aiming for all pairs of latents to have a factorized support.
Specifically, we encourage a factorized support by minimizing a Hausdorff set-distance between the finite sample approximation of the actual support and its factorization (c.f. Huttenlocher et al. 1993, Rockafellar et al. 1998).




# Experiments
Across large-scale experiments on standard disentanglement benchmarks and novel extensions with correlated factors, __HFS__ consistently facilitates disentanglement.
We also show that __HFS__ can be implemented as a regularizer alongside other disentanglement methods to reliably improve disentanglement.
In particular, we observe significant improvements of in parts over $+60\%$ in disentanglement performance over baselines as measured by DCI-D (Eastwood et al. 2018)

</br>

![text](/publications/ICLR23_Hausdorff/main_table.png)

</br>

In addition, on downstream classification tasks, we show improved generalization to more severe distribution shifts as well as improved sample efficiency.

</br>

![text](/publications/ICLR23_Hausdorff/ood_corr_matrix_figure.png)

</br>

</br>

![text](/publications/ICLR23_Hausdorff/relative_improvement_gbt.png)

</br>
